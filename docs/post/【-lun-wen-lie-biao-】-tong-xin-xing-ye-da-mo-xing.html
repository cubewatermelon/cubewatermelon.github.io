<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    <script src='https://blog.meekdai.com/Gmeek/plugins/GmeekVercount.js'></script>
    <link rel="icon" href="https://s21.ax1x.com/2025/04/15/pEf0vaq.jpg"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="### 光通信大模型
- Abishek, A., Adanza, D., Alemany, P., Gifre, L., Casellas, R., Martínez, R., Muñoz, R. and Vilalta, R., 2025. **End-to-end transport network digital twins with cloud-native SDN controllers and generative AI**. Journal of Optical Communications and Networking, 17(7), pp.C70-C81. DT和LLM在SDN中部署的概述，有比较详细的NDT故事，着重添加了网络层（IP层）的相关部分，并在自己的网络控制器TeraFlow中部署；有比较详细的各个功能的介绍，尤其是LLM和intent相关；是很多前述论文的集合体
- 
### 无线通信大模型
- Wu D, Wang X, Qiao Y, Wang Z, Jiang J, Cui S, Wang F. **_NetLLM: Adapting Large Language Models for Networking_**.  Proceedings of the ACM SIGCOMM 2024 Conference; 2024; 2024. p. 661-678. 顶会
- Maatouk A, Ayed F, Piovesan N, De Domenico A, Debbah M, Luo Z-Q. **_Teleqna: A benchmark dataset to assess large language models telecommunications knowledge._** arXiv preprint arXiv:231015051 2023. 通信数据集
- Xu S, Thomas CK, Hashash O, Muralidhar N, Saad W, Ramakrishnan N. **_Large multi-modal models (LMMs) as universal foundation models for AI-native wireless systems_**. arXiv preprint arXiv:240201748 2024. 多模态
- Fontaine J, Shahid A, De Poorter E. **_Towards a Wireless Physical-Layer Foundation Model: Challenges and Strategies_**. arXiv preprint arXiv:240312065 2024. 基础大模型
- Liu C, Xie X, Zhang X, Cui Y. **_Large Language Models for Networking: Workflow, Advances and Challenges_**. arXiv preprint arXiv:240412901 2024.
- Zhou H, Hu C, Yuan Y, Cui Y, Jin Y, Chen C, Wu H, Yuan D, Jiang L, Wu D. **_Large language model (llm) for telecommunications: A comprehensive survey on principles, key techniques, and opportunities_**. arXiv preprint arXiv:240510825 2024.
- Khoramnejad F, Hossain E. **_Generative AI for the Optimization of Next-Generation Wireless Networks: Basics, State-of-the-Art, and Open Challenges_**. IEEE Communications Surveys & Tutorials 2025: 1-1. 综述
- Chen H, Deng W, Yang S, Xu J, Jiang Z, Ngai ECH, Liu J, Liu X. **_Towards Edge General Intelligence via Large Language Models: Opportunities and Challenges_**. IEEE Network 2025: 1-1.
- Chen Z, Sun Q, Li N, Li X, Wang Y, Chih-Lin I. **_Enabling Mobile AI Agent in 6G Era: Architecture and Key Technologies_**. IEEE Network 2024: 1-1.
- Huang Y, Du H, Zhang X, Niyato D, Kang J, Xiong Z, Wang S, Huang T. **_Large language models for networking: Applications, enabling techniques, and challenges_**. IEEE Network 2024.
- Wang Z, Zhou Y, Shi Y, Letaief KB. **_Federated Fine-Tuning for Pre-Trained Foundation Models Over Wireless Networks_**. IEEE Transactions on Wireless Communications 2025: 1-1.
- Du J, Lin T, Jiang C, Yang Q, Bader CF, Han Z. **_Distributed Foundation Models for Multi-Modal Learning in 6G Wireless Networks_**. IEEE Wireless Communications 2024, 31(3): 20-30.
- Chen Z, Zhang Z, Yang Z. Big AI Models for 6G Wireless Networks: Opportunities, Challenges, and Research Directions. IEEE Wireless Communications 2024, 31(5): 164-172.

### 数理基础大模型
- Subramanian S, Harrington P, Keutzer K, Bhimji W, Morozov D, Mahoney MW, Gholami A. **_Towards foundation models for scientific machine learning: Characterizing scaling and transfer behavior_**. Advances in Neural Information Processing Systems 2024, 36. 科学计算大模型，分析了不同因素的影响
- Ye Z, Huang X, Chen L, Liu H, Wang Z, Dong B. **_Pdeformer: Towards a foundation model for one-dimensional partial differential equations_**. arXiv preprint arXiv:240212652 2024. 用符号的形式表征了PDE
- Hao Z, Su C, Liu S, Berner J, Ying C, Su H, Anandkumar A, Song J, Zhu J. **_Dpot: Auto-regressive denoising operator transformer for large-scale pde pre-training_**. arXiv preprint arXiv:240303542 2024. 好文
- Wang S, Seidman JH, Sankaran S, Wang H, Pappas GJ, Perdikaris P. **_Bridging Operator Learning and Conditioned Neural Fields: A Unifying Perspective_**. arXiv preprint arXiv:240513998 2024. 好文，借鉴了DeepONet结构思想，开源代码运行比较顺畅
- Hang Z, Ma Y, Wu H, Wang H, Long M. **_Unisolver: PDE-Conditional Transformers Are Universal PDE Solvers_**. arXiv preprint arXiv:240517527 2024. 原理上感觉比较科学
- Herde M, Raonić B, Rohner T, Käppeli R, Molinaro R, de Bézenac E, Mishra S. Poseidon: Efficient Foundation Models for PDEs. arXiv preprint arXiv:240519101 2024.
- Hao Z, Wang Z, Su H, Ying C, Dong Y, Liu S, Cheng Z, Song J, Zhu J. Gnot: A general neural operator transformer for operator learning.  International Conference on Machine Learning; 2023: PMLR; 2023. p. 12556-12569.

### 时序基础大模型
- Bommasani R, Hudson DA, Adeli E, Altman R, Arora S, von Arx S, Bernstein MS, Bohg J, Bosselut A, Brunskill E. **_On the opportunities and risks of foundation models_**. arXiv preprint arXiv:210807258 2021. 基础模型
- Chang C, Peng W-C, Chen T-F. **_Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms_**. arXiv preprint arXiv:230808469 2023.
- Jin M, Wang S, Ma L, Chu Z, Zhang JY, Shi X, Chen P-Y, Liang Y, Li Y-F, Pan S. **_Time-llm: Time series forecasting by reprogramming large language models_**. arXiv preprint arXiv:231001728 2023.
- Xu M, Yin W, Cai D, Yi R, Xu D, Wang Q, Wu B, Zhao Y, Yang C, Wang S. **_A survey of resource-efficient llm and multimodal foundation models_**. arXiv preprint arXiv:240108092 2024.
- Wang Q, Qian C, Li X, Yao Z, Shao H. **_Lens: A Foundation Model for Network Traffic_**. arXiv preprint arXiv:240203646 2024.
- Darlow L, Deng Q, Hassan A, Asenov M, Singh R, Joosen A, Barker A, Storkey A. **_DAM: Towards A Foundation Model for Time Series Forecasting_**. arXiv preprint arXiv:240717880 2024.
- Li C, Gan Z, Yang Z, Yang J, Li L, Wang L, Gao J. **_Multimodal foundation models: From specialists to general-purpose assistants_**. Foundations and Trends® in Computer Graphics and Vision 2024, 16(1-2): 1-214.
 -Liang Y, Wen H, Nie Y, Jiang Y, Jin M, Song D, Pan S, Wen Q. **_Foundation Models for Time Series Analysis: A Tutorial and Survey_**.  Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining; 2024. pp. 6555-6565.

### 可能有用的大模型领域洞察
- Xiao C, Cai J, Zhao W, Zeng G, Han X, Liu Z, Sun M. **_Densing Law of LLMs_**. arXiv preprint arXiv:241204315 2024. 探讨了大模型能力密度的关系，从训练模型大小、轮数、数据量大小、参数量大小等可讨论角度上说比较有启发
- Michaud EJ, Liao I, Lad V, Liu Z, Mudide A, Loughridge C, Guo ZC, Kheirkhah TR, Vukelić M, Tegmark M. **_Opening the AI Black Box: Distilling Machine-Learned Algorithms into Code_**. Entropy 2024, 26(12). 将算法转化为代码的初步讨论
- Farquhar S, Kossen J, Kuhn L, Gal Y. **_Detecting hallucinations in large language models using semantic entropy_**. Nature 2024, 630(8017): 625-630. 大模型回答是否靠谱的语义上的分析
- Marcondes D, Simonis A, Barrera J. **_Back to basics to open the black box_**. Nature Machine Intelligence 2024, 6(5): 498-501. 评论

### 大模型和通信基础设施
- Qian K, Xi Y, Cao J, Gao J, Xu Y, Guan Y, Fu B, Shi X, Zhu F, Miao R. **_Alibaba HPN: A Data Center Network for Large Language Model Training_**. Traffic 2024, 1: 2. 分析了大模型在训练过程中的一些流量特征，包含了大模型的一些基础知识
- Hu Q, Ye Z, Wang Z, Wang G, Zhang M, Chen Q, Sun P, Lin D, Wang X, Luo Y. **_Characterization of large language model development in the datacenter_**.  21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24); 2024; 2024. p. 709-729.
- Poutievski L, Mashayekhi O, Ong J, Singh A, Tariq M, Wang R, Zhang J, Beauregard V, Conner P, Gribble S, Kapoor R, Kratzer S, Li N, Liu H, Nagaraj K, Ornstein J, Sawhney S, Urata R, Vicisano L, Yasumura K, Zhang S, Zhou J, Vahdat A. **_Jupiter evolving_**.  Proceedings of the ACM SIGCOMM 2022 Conference; 2022. pp. 66-85.
- Liu H, Urata R, Yasumura K, Zhou X, Bannon R, Berger J, Dashti P, Jouppi N, Lam C, Li S, Mao E, Nelson D, Papen G, Tariq M, Vahdat A. **_Lightwave Fabrics: At-Scale Optical Circuit Switching for Datacenter and Machine Learning Systems_**.  Proceedings of the ACM SIGCOMM 2023 Conference; 2023. pp. 499-515.

### 大模型与科学发现
- Romera-Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar, M.P., Dupont, E., Ruiz, F.J., Ellenberg, J.S., Wang, P., Fawzi, O. and Kohli, P., 2024. **_Mathematical discoveries from program search with large language models_**. Nature, 625(7995), pp.468-475. 通过大模型的不断选择迭代代码库实现算法的升级
- Du M, Chen Y, Wang Z, Nie L, Zhang D. **_LLM4ED: Large Language Models for Automatic Equation Discovery_**. arXiv preprint arXiv:240507761 2024.
- Ma P, Wang T-H, Guo M, Sun Z, Tenenbaum JB, Rus D, Gan C, Matusik W. **_LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery_**. arXiv preprint arXiv:240509783 2024.
。">
<meta property="og:title" content="【论文列表】通信行业大模型">
<meta property="og:description" content="### 光通信大模型
- Abishek, A., Adanza, D., Alemany, P., Gifre, L., Casellas, R., Martínez, R., Muñoz, R. and Vilalta, R., 2025. **End-to-end transport network digital twins with cloud-native SDN controllers and generative AI**. Journal of Optical Communications and Networking, 17(7), pp.C70-C81. DT和LLM在SDN中部署的概述，有比较详细的NDT故事，着重添加了网络层（IP层）的相关部分，并在自己的网络控制器TeraFlow中部署；有比较详细的各个功能的介绍，尤其是LLM和intent相关；是很多前述论文的集合体
- 
### 无线通信大模型
- Wu D, Wang X, Qiao Y, Wang Z, Jiang J, Cui S, Wang F. **_NetLLM: Adapting Large Language Models for Networking_**.  Proceedings of the ACM SIGCOMM 2024 Conference; 2024; 2024. p. 661-678. 顶会
- Maatouk A, Ayed F, Piovesan N, De Domenico A, Debbah M, Luo Z-Q. **_Teleqna: A benchmark dataset to assess large language models telecommunications knowledge._** arXiv preprint arXiv:231015051 2023. 通信数据集
- Xu S, Thomas CK, Hashash O, Muralidhar N, Saad W, Ramakrishnan N. **_Large multi-modal models (LMMs) as universal foundation models for AI-native wireless systems_**. arXiv preprint arXiv:240201748 2024. 多模态
- Fontaine J, Shahid A, De Poorter E. **_Towards a Wireless Physical-Layer Foundation Model: Challenges and Strategies_**. arXiv preprint arXiv:240312065 2024. 基础大模型
- Liu C, Xie X, Zhang X, Cui Y. **_Large Language Models for Networking: Workflow, Advances and Challenges_**. arXiv preprint arXiv:240412901 2024.
- Zhou H, Hu C, Yuan Y, Cui Y, Jin Y, Chen C, Wu H, Yuan D, Jiang L, Wu D. **_Large language model (llm) for telecommunications: A comprehensive survey on principles, key techniques, and opportunities_**. arXiv preprint arXiv:240510825 2024.
- Khoramnejad F, Hossain E. **_Generative AI for the Optimization of Next-Generation Wireless Networks: Basics, State-of-the-Art, and Open Challenges_**. IEEE Communications Surveys & Tutorials 2025: 1-1. 综述
- Chen H, Deng W, Yang S, Xu J, Jiang Z, Ngai ECH, Liu J, Liu X. **_Towards Edge General Intelligence via Large Language Models: Opportunities and Challenges_**. IEEE Network 2025: 1-1.
- Chen Z, Sun Q, Li N, Li X, Wang Y, Chih-Lin I. **_Enabling Mobile AI Agent in 6G Era: Architecture and Key Technologies_**. IEEE Network 2024: 1-1.
- Huang Y, Du H, Zhang X, Niyato D, Kang J, Xiong Z, Wang S, Huang T. **_Large language models for networking: Applications, enabling techniques, and challenges_**. IEEE Network 2024.
- Wang Z, Zhou Y, Shi Y, Letaief KB. **_Federated Fine-Tuning for Pre-Trained Foundation Models Over Wireless Networks_**. IEEE Transactions on Wireless Communications 2025: 1-1.
- Du J, Lin T, Jiang C, Yang Q, Bader CF, Han Z. **_Distributed Foundation Models for Multi-Modal Learning in 6G Wireless Networks_**. IEEE Wireless Communications 2024, 31(3): 20-30.
- Chen Z, Zhang Z, Yang Z. Big AI Models for 6G Wireless Networks: Opportunities, Challenges, and Research Directions. IEEE Wireless Communications 2024, 31(5): 164-172.

### 数理基础大模型
- Subramanian S, Harrington P, Keutzer K, Bhimji W, Morozov D, Mahoney MW, Gholami A. **_Towards foundation models for scientific machine learning: Characterizing scaling and transfer behavior_**. Advances in Neural Information Processing Systems 2024, 36. 科学计算大模型，分析了不同因素的影响
- Ye Z, Huang X, Chen L, Liu H, Wang Z, Dong B. **_Pdeformer: Towards a foundation model for one-dimensional partial differential equations_**. arXiv preprint arXiv:240212652 2024. 用符号的形式表征了PDE
- Hao Z, Su C, Liu S, Berner J, Ying C, Su H, Anandkumar A, Song J, Zhu J. **_Dpot: Auto-regressive denoising operator transformer for large-scale pde pre-training_**. arXiv preprint arXiv:240303542 2024. 好文
- Wang S, Seidman JH, Sankaran S, Wang H, Pappas GJ, Perdikaris P. **_Bridging Operator Learning and Conditioned Neural Fields: A Unifying Perspective_**. arXiv preprint arXiv:240513998 2024. 好文，借鉴了DeepONet结构思想，开源代码运行比较顺畅
- Hang Z, Ma Y, Wu H, Wang H, Long M. **_Unisolver: PDE-Conditional Transformers Are Universal PDE Solvers_**. arXiv preprint arXiv:240517527 2024. 原理上感觉比较科学
- Herde M, Raonić B, Rohner T, Käppeli R, Molinaro R, de Bézenac E, Mishra S. Poseidon: Efficient Foundation Models for PDEs. arXiv preprint arXiv:240519101 2024.
- Hao Z, Wang Z, Su H, Ying C, Dong Y, Liu S, Cheng Z, Song J, Zhu J. Gnot: A general neural operator transformer for operator learning.  International Conference on Machine Learning; 2023: PMLR; 2023. p. 12556-12569.

### 时序基础大模型
- Bommasani R, Hudson DA, Adeli E, Altman R, Arora S, von Arx S, Bernstein MS, Bohg J, Bosselut A, Brunskill E. **_On the opportunities and risks of foundation models_**. arXiv preprint arXiv:210807258 2021. 基础模型
- Chang C, Peng W-C, Chen T-F. **_Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms_**. arXiv preprint arXiv:230808469 2023.
- Jin M, Wang S, Ma L, Chu Z, Zhang JY, Shi X, Chen P-Y, Liang Y, Li Y-F, Pan S. **_Time-llm: Time series forecasting by reprogramming large language models_**. arXiv preprint arXiv:231001728 2023.
- Xu M, Yin W, Cai D, Yi R, Xu D, Wang Q, Wu B, Zhao Y, Yang C, Wang S. **_A survey of resource-efficient llm and multimodal foundation models_**. arXiv preprint arXiv:240108092 2024.
- Wang Q, Qian C, Li X, Yao Z, Shao H. **_Lens: A Foundation Model for Network Traffic_**. arXiv preprint arXiv:240203646 2024.
- Darlow L, Deng Q, Hassan A, Asenov M, Singh R, Joosen A, Barker A, Storkey A. **_DAM: Towards A Foundation Model for Time Series Forecasting_**. arXiv preprint arXiv:240717880 2024.
- Li C, Gan Z, Yang Z, Yang J, Li L, Wang L, Gao J. **_Multimodal foundation models: From specialists to general-purpose assistants_**. Foundations and Trends® in Computer Graphics and Vision 2024, 16(1-2): 1-214.
 -Liang Y, Wen H, Nie Y, Jiang Y, Jin M, Song D, Pan S, Wen Q. **_Foundation Models for Time Series Analysis: A Tutorial and Survey_**.  Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining; 2024. pp. 6555-6565.

### 可能有用的大模型领域洞察
- Xiao C, Cai J, Zhao W, Zeng G, Han X, Liu Z, Sun M. **_Densing Law of LLMs_**. arXiv preprint arXiv:241204315 2024. 探讨了大模型能力密度的关系，从训练模型大小、轮数、数据量大小、参数量大小等可讨论角度上说比较有启发
- Michaud EJ, Liao I, Lad V, Liu Z, Mudide A, Loughridge C, Guo ZC, Kheirkhah TR, Vukelić M, Tegmark M. **_Opening the AI Black Box: Distilling Machine-Learned Algorithms into Code_**. Entropy 2024, 26(12). 将算法转化为代码的初步讨论
- Farquhar S, Kossen J, Kuhn L, Gal Y. **_Detecting hallucinations in large language models using semantic entropy_**. Nature 2024, 630(8017): 625-630. 大模型回答是否靠谱的语义上的分析
- Marcondes D, Simonis A, Barrera J. **_Back to basics to open the black box_**. Nature Machine Intelligence 2024, 6(5): 498-501. 评论

### 大模型和通信基础设施
- Qian K, Xi Y, Cao J, Gao J, Xu Y, Guan Y, Fu B, Shi X, Zhu F, Miao R. **_Alibaba HPN: A Data Center Network for Large Language Model Training_**. Traffic 2024, 1: 2. 分析了大模型在训练过程中的一些流量特征，包含了大模型的一些基础知识
- Hu Q, Ye Z, Wang Z, Wang G, Zhang M, Chen Q, Sun P, Lin D, Wang X, Luo Y. **_Characterization of large language model development in the datacenter_**.  21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24); 2024; 2024. p. 709-729.
- Poutievski L, Mashayekhi O, Ong J, Singh A, Tariq M, Wang R, Zhang J, Beauregard V, Conner P, Gribble S, Kapoor R, Kratzer S, Li N, Liu H, Nagaraj K, Ornstein J, Sawhney S, Urata R, Vicisano L, Yasumura K, Zhang S, Zhou J, Vahdat A. **_Jupiter evolving_**.  Proceedings of the ACM SIGCOMM 2022 Conference; 2022. pp. 66-85.
- Liu H, Urata R, Yasumura K, Zhou X, Bannon R, Berger J, Dashti P, Jouppi N, Lam C, Li S, Mao E, Nelson D, Papen G, Tariq M, Vahdat A. **_Lightwave Fabrics: At-Scale Optical Circuit Switching for Datacenter and Machine Learning Systems_**.  Proceedings of the ACM SIGCOMM 2023 Conference; 2023. pp. 499-515.

### 大模型与科学发现
- Romera-Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar, M.P., Dupont, E., Ruiz, F.J., Ellenberg, J.S., Wang, P., Fawzi, O. and Kohli, P., 2024. **_Mathematical discoveries from program search with large language models_**. Nature, 625(7995), pp.468-475. 通过大模型的不断选择迭代代码库实现算法的升级
- Du M, Chen Y, Wang Z, Nie L, Zhang D. **_LLM4ED: Large Language Models for Automatic Equation Discovery_**. arXiv preprint arXiv:240507761 2024.
- Ma P, Wang T-H, Guo M, Sun Z, Tenenbaum JB, Rus D, Gan C, Matusik W. **_LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery_**. arXiv preprint arXiv:240509783 2024.
。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://cubewatermelon.github.io/post/%E3%80%90-lun-wen-lie-biao-%E3%80%91-tong-xin-xing-ye-da-mo-xing.html">
<meta property="og:image" content="https://s21.ax1x.com/2025/04/15/pEf0vaq.jpg">
<title>【论文列表】通信行业大模型</title>



</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}

</style>




<body>
    <div id="header">
<h1 class="postTitle">【论文列表】通信行业大模型</h1>
<div class="title-right">
    <a href="https://cubewatermelon.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/cubewatermelon/cubewatermelon.github.io/issues/10" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><h3>光通信大模型</h3>
<ul>
<li>Abishek, A., Adanza, D., Alemany, P., Gifre, L., Casellas, R., Martínez, R., Muñoz, R. and Vilalta, R., 2025. <strong>End-to-end transport network digital twins with cloud-native SDN controllers and generative AI</strong>. Journal of Optical Communications and Networking, 17(7), pp.C70-C81. DT和LLM在SDN中部署的概述，有比较详细的NDT故事，着重添加了网络层（IP层）的相关部分，并在自己的网络控制器TeraFlow中部署；有比较详细的各个功能的介绍，尤其是LLM和intent相关；是很多前述论文的集合体</li>
<li></li>
</ul>
<h3>无线通信大模型</h3>
<ul>
<li>Wu D, Wang X, Qiao Y, Wang Z, Jiang J, Cui S, Wang F. <strong><em>NetLLM: Adapting Large Language Models for Networking</em></strong>.  Proceedings of the ACM SIGCOMM 2024 Conference; 2024; 2024. p. 661-678. 顶会</li>
<li>Maatouk A, Ayed F, Piovesan N, De Domenico A, Debbah M, Luo Z-Q. <strong><em>Teleqna: A benchmark dataset to assess large language models telecommunications knowledge.</em></strong> arXiv preprint arXiv:231015051 2023. 通信数据集</li>
<li>Xu S, Thomas CK, Hashash O, Muralidhar N, Saad W, Ramakrishnan N. <strong><em>Large multi-modal models (LMMs) as universal foundation models for AI-native wireless systems</em></strong>. arXiv preprint arXiv:240201748 2024. 多模态</li>
<li>Fontaine J, Shahid A, De Poorter E. <strong><em>Towards a Wireless Physical-Layer Foundation Model: Challenges and Strategies</em></strong>. arXiv preprint arXiv:240312065 2024. 基础大模型</li>
<li>Liu C, Xie X, Zhang X, Cui Y. <strong><em>Large Language Models for Networking: Workflow, Advances and Challenges</em></strong>. arXiv preprint arXiv:240412901 2024.</li>
<li>Zhou H, Hu C, Yuan Y, Cui Y, Jin Y, Chen C, Wu H, Yuan D, Jiang L, Wu D. <strong><em>Large language model (llm) for telecommunications: A comprehensive survey on principles, key techniques, and opportunities</em></strong>. arXiv preprint arXiv:240510825 2024.</li>
<li>Khoramnejad F, Hossain E. <strong><em>Generative AI for the Optimization of Next-Generation Wireless Networks: Basics, State-of-the-Art, and Open Challenges</em></strong>. IEEE Communications Surveys &amp; Tutorials 2025: 1-1. 综述</li>
<li>Chen H, Deng W, Yang S, Xu J, Jiang Z, Ngai ECH, Liu J, Liu X. <strong><em>Towards Edge General Intelligence via Large Language Models: Opportunities and Challenges</em></strong>. IEEE Network 2025: 1-1.</li>
<li>Chen Z, Sun Q, Li N, Li X, Wang Y, Chih-Lin I. <strong><em>Enabling Mobile AI Agent in 6G Era: Architecture and Key Technologies</em></strong>. IEEE Network 2024: 1-1.</li>
<li>Huang Y, Du H, Zhang X, Niyato D, Kang J, Xiong Z, Wang S, Huang T. <strong><em>Large language models for networking: Applications, enabling techniques, and challenges</em></strong>. IEEE Network 2024.</li>
<li>Wang Z, Zhou Y, Shi Y, Letaief KB. <strong><em>Federated Fine-Tuning for Pre-Trained Foundation Models Over Wireless Networks</em></strong>. IEEE Transactions on Wireless Communications 2025: 1-1.</li>
<li>Du J, Lin T, Jiang C, Yang Q, Bader CF, Han Z. <strong><em>Distributed Foundation Models for Multi-Modal Learning in 6G Wireless Networks</em></strong>. IEEE Wireless Communications 2024, 31(3): 20-30.</li>
<li>Chen Z, Zhang Z, Yang Z. Big AI Models for 6G Wireless Networks: Opportunities, Challenges, and Research Directions. IEEE Wireless Communications 2024, 31(5): 164-172.</li>
</ul>
<h3>数理基础大模型</h3>
<ul>
<li>Subramanian S, Harrington P, Keutzer K, Bhimji W, Morozov D, Mahoney MW, Gholami A. <strong><em>Towards foundation models for scientific machine learning: Characterizing scaling and transfer behavior</em></strong>. Advances in Neural Information Processing Systems 2024, 36. 科学计算大模型，分析了不同因素的影响</li>
<li>Ye Z, Huang X, Chen L, Liu H, Wang Z, Dong B. <strong><em>Pdeformer: Towards a foundation model for one-dimensional partial differential equations</em></strong>. arXiv preprint arXiv:240212652 2024. 用符号的形式表征了PDE</li>
<li>Hao Z, Su C, Liu S, Berner J, Ying C, Su H, Anandkumar A, Song J, Zhu J. <strong><em>Dpot: Auto-regressive denoising operator transformer for large-scale pde pre-training</em></strong>. arXiv preprint arXiv:240303542 2024. 好文</li>
<li>Wang S, Seidman JH, Sankaran S, Wang H, Pappas GJ, Perdikaris P. <strong><em>Bridging Operator Learning and Conditioned Neural Fields: A Unifying Perspective</em></strong>. arXiv preprint arXiv:240513998 2024. 好文，借鉴了DeepONet结构思想，开源代码运行比较顺畅</li>
<li>Hang Z, Ma Y, Wu H, Wang H, Long M. <strong><em>Unisolver: PDE-Conditional Transformers Are Universal PDE Solvers</em></strong>. arXiv preprint arXiv:240517527 2024. 原理上感觉比较科学</li>
<li>Herde M, Raonić B, Rohner T, Käppeli R, Molinaro R, de Bézenac E, Mishra S. Poseidon: Efficient Foundation Models for PDEs. arXiv preprint arXiv:240519101 2024.</li>
<li>Hao Z, Wang Z, Su H, Ying C, Dong Y, Liu S, Cheng Z, Song J, Zhu J. Gnot: A general neural operator transformer for operator learning.  International Conference on Machine Learning; 2023: PMLR; 2023. p. 12556-12569.</li>
</ul>
<h3>时序基础大模型</h3>
<ul>
<li>Bommasani R, Hudson DA, Adeli E, Altman R, Arora S, von Arx S, Bernstein MS, Bohg J, Bosselut A, Brunskill E. <strong><em>On the opportunities and risks of foundation models</em></strong>. arXiv preprint arXiv:210807258 2021. 基础模型</li>
<li>Chang C, Peng W-C, Chen T-F. <strong><em>Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms</em></strong>. arXiv preprint arXiv:230808469 2023.</li>
<li>Jin M, Wang S, Ma L, Chu Z, Zhang JY, Shi X, Chen P-Y, Liang Y, Li Y-F, Pan S. <strong><em>Time-llm: Time series forecasting by reprogramming large language models</em></strong>. arXiv preprint arXiv:231001728 2023.</li>
<li>Xu M, Yin W, Cai D, Yi R, Xu D, Wang Q, Wu B, Zhao Y, Yang C, Wang S. <strong><em>A survey of resource-efficient llm and multimodal foundation models</em></strong>. arXiv preprint arXiv:240108092 2024.</li>
<li>Wang Q, Qian C, Li X, Yao Z, Shao H. <strong><em>Lens: A Foundation Model for Network Traffic</em></strong>. arXiv preprint arXiv:240203646 2024.</li>
<li>Darlow L, Deng Q, Hassan A, Asenov M, Singh R, Joosen A, Barker A, Storkey A. <strong><em>DAM: Towards A Foundation Model for Time Series Forecasting</em></strong>. arXiv preprint arXiv:240717880 2024.</li>
<li>Li C, Gan Z, Yang Z, Yang J, Li L, Wang L, Gao J. <strong><em>Multimodal foundation models: From specialists to general-purpose assistants</em></strong>. Foundations and Trends® in Computer Graphics and Vision 2024, 16(1-2): 1-214.<br>
-Liang Y, Wen H, Nie Y, Jiang Y, Jin M, Song D, Pan S, Wen Q. <strong><em>Foundation Models for Time Series Analysis: A Tutorial and Survey</em></strong>.  Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining; 2024. pp. 6555-6565.</li>
</ul>
<h3>可能有用的大模型领域洞察</h3>
<ul>
<li>Xiao C, Cai J, Zhao W, Zeng G, Han X, Liu Z, Sun M. <strong><em>Densing Law of LLMs</em></strong>. arXiv preprint arXiv:241204315 2024. 探讨了大模型能力密度的关系，从训练模型大小、轮数、数据量大小、参数量大小等可讨论角度上说比较有启发</li>
<li>Michaud EJ, Liao I, Lad V, Liu Z, Mudide A, Loughridge C, Guo ZC, Kheirkhah TR, Vukelić M, Tegmark M. <strong><em>Opening the AI Black Box: Distilling Machine-Learned Algorithms into Code</em></strong>. Entropy 2024, 26(12). 将算法转化为代码的初步讨论</li>
<li>Farquhar S, Kossen J, Kuhn L, Gal Y. <strong><em>Detecting hallucinations in large language models using semantic entropy</em></strong>. Nature 2024, 630(8017): 625-630. 大模型回答是否靠谱的语义上的分析</li>
<li>Marcondes D, Simonis A, Barrera J. <strong><em>Back to basics to open the black box</em></strong>. Nature Machine Intelligence 2024, 6(5): 498-501. 评论</li>
</ul>
<h3>大模型和通信基础设施</h3>
<ul>
<li>Qian K, Xi Y, Cao J, Gao J, Xu Y, Guan Y, Fu B, Shi X, Zhu F, Miao R. <strong><em>Alibaba HPN: A Data Center Network for Large Language Model Training</em></strong>. Traffic 2024, 1: 2. 分析了大模型在训练过程中的一些流量特征，包含了大模型的一些基础知识</li>
<li>Hu Q, Ye Z, Wang Z, Wang G, Zhang M, Chen Q, Sun P, Lin D, Wang X, Luo Y. <strong><em>Characterization of large language model development in the datacenter</em></strong>.  21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24); 2024; 2024. p. 709-729.</li>
<li>Poutievski L, Mashayekhi O, Ong J, Singh A, Tariq M, Wang R, Zhang J, Beauregard V, Conner P, Gribble S, Kapoor R, Kratzer S, Li N, Liu H, Nagaraj K, Ornstein J, Sawhney S, Urata R, Vicisano L, Yasumura K, Zhang S, Zhou J, Vahdat A. <strong><em>Jupiter evolving</em></strong>.  Proceedings of the ACM SIGCOMM 2022 Conference; 2022. pp. 66-85.</li>
<li>Liu H, Urata R, Yasumura K, Zhou X, Bannon R, Berger J, Dashti P, Jouppi N, Lam C, Li S, Mao E, Nelson D, Papen G, Tariq M, Vahdat A. <strong><em>Lightwave Fabrics: At-Scale Optical Circuit Switching for Datacenter and Machine Learning Systems</em></strong>.  Proceedings of the ACM SIGCOMM 2023 Conference; 2023. pp. 499-515.</li>
</ul>
<h3>大模型与科学发现</h3>
<ul>
<li>Romera-Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar, M.P., Dupont, E., Ruiz, F.J., Ellenberg, J.S., Wang, P., Fawzi, O. and Kohli, P., 2024. <strong><em>Mathematical discoveries from program search with large language models</em></strong>. Nature, 625(7995), pp.468-475. 通过大模型的不断选择迭代代码库实现算法的升级</li>
<li>Du M, Chen Y, Wang Z, Nie L, Zhang D. <strong><em>LLM4ED: Large Language Models for Automatic Equation Discovery</em></strong>. arXiv preprint arXiv:240507761 2024.</li>
<li>Ma P, Wang T-H, Guo M, Sun Z, Tenenbaum JB, Rus D, Gan C, Matusik W. <strong><em>LLM and Simulation as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Discovery</em></strong>. arXiv preprint arXiv:240509783 2024.</li>
</ul></div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer"><div id="footer1">Copyright © <span id="copyrightYear"></span> <a href="https://cubewatermelon.github.io">Yuchen's Blog</a></div>
<div id="footer2">
    <span id="runday"></span><span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a></span>
</div>

<script>
var now=new Date();
document.getElementById("copyrightYear").innerHTML=now.getFullYear();

if(""!=""){
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";
}
</script></div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);



function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","cubewatermelon/cubewatermelon.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}



</script>


</html>
