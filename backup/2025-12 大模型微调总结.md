## 大模型快速微调
- 鉴于小作坊难以实现大规模系统化大模型垂域微调，借鉴Deepseek的经验路径，通过轻量化SFT+重度RL方法来实现大模型的高质量策略回答
- 可以这么做的原因一方面是我们关注的问题并不通用，回答模式上较为狭窄；另一方面是reward可以根据策略回答通过程序的方式，非常明确的计算出来，即“可靠、自动、低噪声的 reward”
- 其中SFT阶段不追求输出策略上的极高准确性，通过少量数据，确保模型能稳定输出 可执行 / 可验证 的东西
- 马上进入RL微调阶段，通过大量数据以及长时间的RL微调（结合相对较快的off-policy和相对而言慢的on-policy），来不断提升模型性能

### RL微调
- GRPO方法通过在每一轮训练中，比较针对同一prompt的多个输出的相对得分，来引导模型的训练
- 每一轮需要多次调用仿真工具得到奖励分数，运行时间较慢
- 本质上在计算πθ​(y∣x)，其中θ是模型的参数，πθ是在该参数下的模型（算子），x是输入prompt，y是输出，可以是response中每个token的输出概率，也可以是整个response输出的联合概率分布
- GRPO在训练过程中，由于每一组生成回答间相对的不稳定性等原因，reward趋势也呈现较强的不稳定性，导致较差的训练效果
- 采用Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO)来增强GRPO训练过程中的稳定性
- 在DAPO中采用动态采样、非对称裁剪和token级损失函数来一定程度上提升训练稳定性

### 思维链微调
- 思维链一方面可以提升输出结果的可靠性和可解释性，另一方面在结果有问题，也可以通过思维链来溯源，并针对性的调整模型训练策略