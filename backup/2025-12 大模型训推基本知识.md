## 推理时间分类
从按下回车键到看到完整的回答，推理时间主要可以拆解为以下两个个核心指标：TTFT和TPOT

1. **首字延迟 (TTFT: Time to First Token)**
这是用户感知最明显的指标，指的是从发送请求到模型吐出第一个字符所经历的时间。

处理阶段： 预填充阶段 (Prefilling)。

模型在干什么： 1. 输入处理： 模型需要一次性处理你输入的全部 Prompt。 2. KV Cache 初始计算： 模型将输入中的所有 Token 转换为键值对（Key-Value Pairs）并缓存起来，为后续生成做准备。

影响因素： 输入 Prompt 的长度（输入越长，TTFT 越高）以及服务器的计算算力。

2. **单 Token 生成时间 (TPOT: Time Per Output Token)**
这是模型“说话”过程中的节奏，指的是生成每一个后续 Token 所需的平均时间。

处理阶段： 解码阶段 (Decoding)。模型在干什么：模型根据之前的上下文（利用已经缓存的 KV Cache）预测下一个 Token。这是一个自回归的过程：生成第 $N$ 个 Token 必须依赖于第 $N-1$ 个 Token 的结果，因此只能一个一个地串行生成。

影响因素： 模型参数量大小、显存带宽（内存访问速度决定了每秒能读多少次权重）。

如果 TPOT 是 50ms，意味着模型每秒能生成 20 个 Token。这通常快于人类的阅读速度（人类阅读平均每秒约 5-10 个词），所以会觉得模型回答得很流畅。

## 推理时间受限于什么
**在计算机体系结构中，任务通常分为 计算密集型（受限于 CPU/GPU 的算力 TFLOPS）和 访存密集型（受限于显存带宽 GB/s）。**

**1. 什么是 Prefilling（预填充）阶段？**
大模型推理其实分为两个截然不同的动作：Prefilling（预填充） 和 Decoding（解码）。

它的任务： 当你输入一段话（Prompt）按下回车时，模型的第一步是把你这几百个甚至几千个 Token 一次性“吞”进去。

计算特点： * 并行计算： 因为输入的 Prompt 是已知的，模型可以利用 GPU 的数千个核心，同时计算所有输入 Token 之间的注意力（Attention）。

生成 KV Cache： 这一步最重要的产出是 KV Cache（键值缓存）。模型计算出输入部分的 Key 和 Value 向量并存在显存里，这样以后生成新词时，就不用重复计算旧词的特征了。

用户感知： 这一阶段决定了 TTFT（首字延迟）。

**2. 为什么推理是“访存密集型”任务？**
大模型推理其实分为两个截然不同的动作：Prefilling（预填充） 和 Decoding（解码）。
大模型的解码（Decoding）阶段，是典型的访存密集型任务。 理由如下：

**① “权重搬运”的速度跟不上计算速度**
在生成每一个新 Token 时，GPU 必须把模型的**全部参数（权重）**从显存（HBM）读取到显卡核心（SRAM）中进行一次矩阵运算。

例子： 假设一个 70B（700 亿参数）的模型，使用 FP16 精度，权重大小约为 140GB。

残酷的现实： 为了生成一个 Token，GPU 必须把这 140GB 的数据全部搬运一遍。

瓶颈所在： 现代 GPU（如 H100）的算力非常恐怖，但显存带宽（搬运速度）相对很慢。这就好比你有一台顶级跑车（算力），但油管只有吸管那么粗（带宽），车再快也跑不起来。

**② KV Cache 的存储压力**
随着对话变长，KV Cache 会迅速膨胀。

模型不仅要搬运权重，还要不断读写缓存的 KV 向量。

如果显存带宽不足，模型在查找“之前说过的话”时就会变慢。

**③ 算力利用率极低**
在 Prefilling 阶段，因为是并行处理，算力利用率很高（计算密集）。 但在 Decoding 阶段，因为一次只算一个 Token，GPU 几千个核心里大部分时间都在“等数据从显存搬过来”。此时，决定速度的不是 GPU 有多少核心，而是显存带宽有多大。

## 为什么要用 KV Cache
Transformer 模型在生成每一个新词（Token）时，都需要计算这个新词与前面所有词的“注意力”关系。

如果没有 KV Cache： 模型生成第 101 个词时，需要重新计算前 100 个词的所有特征；生成第 102 个词时，又要重新计算前 101 个词。

后果： 随着句子越写越长，计算量呈几何倍数增长，模型会越写越慢，TPOT（单字耗时）会变得无法忍受。

有了 KV Cache： 工程师发现，在生成过程中，前文 Token 的 Key（键）向量和 Value（值）向量是固定不变的。

做法： 把已经计算过的 Token 的 K 和 V 向量直接存在显存里。

结果： 生成新词时，只需要计算当前这一个词的 K 和 V，然后去缓存里“找”旧的向量直接拼起来用。

KV Cache 的本质就是“空间换时间”： 工程师通过在显存中开辟巨大的缓存空间，避免了重复的数学计算，从而将大模型的吐字速度（TPOT）维持在一个人类阅读可以接受的舒适区间