大型语言模型 (LLM) 的后训练 (Post-training) 通常依赖于两种主流范式：监督微调(Supervised Fine-Tuning, SFT) 和强化学习 (Reinforcement Learning, RL)。SFT 主要通过模仿高质量的专家数据来学习，而 RL 则通过与环境的交互和反馈进行探索性学习。

### PPO（Proximal Policy Optimization，近端策略优化）
- 使用高质量的数据集对预训练的大模型进行有监督微调 (SFT)，使其具备基本的指令遵循能力，得到一个基准策略模型 $\pi_{\text{SFT}}$
- 收集大量的人类偏好数据，即对于同一个 Prompt，让人类对模型生成的不同回答进行评分或比较；基于这些人类偏好，训练一个奖励模型 (RM) $r_{\phi}(\mathbf{x}, \mathbf{y})$。这个模型的作用是预测人类会对任何给定的模型回答 $\mathbf{y}$ 打出多高的“奖励”分数
具体运行方式
1. 模型 $\pi_{\theta}$ 在 Prompt 上生成回答（采样）。
2. 奖励模型 $r_{\phi}$ 对生成的回答进行评分（计算奖励）。
3. PPO 使用这个奖励分数作为回报，通过近端策略优化目标函数来更新模型的参数 $\theta$。
- 为了防止模型在最大化奖励的过程中偏离原始 SFT 模型太远（导致生成不连贯或怪异的文本），PPO 的目标函数中加入了一个 KL 散度惩罚项，确保新策略 $\pi_{\theta}$ 与基准策略 $\pi_{\text{SFT}}$ 保持“接近”。

### DPO（Direct Preference Optimization，直接偏好优化）
- 基本流程与PPO类似，最大区别在于直接使用人类偏好数据 $(\mathbf{x}, \mathbf{y}_w, \mathbf{y}_l)$，其中 $\mathbf{y}_w$ 是优选回答，$\mathbf{y}_l$ 是劣选回答，**通过数据的比较实现微调过程的加速**，加速在算力不足时非常重要
- 它将复杂的强化学习问题转化为了一个简单的、稳定的分类优化问题，避免了训练奖励模型时可能出现的误差累积和 PPO 训练中复杂的采样和梯度估计
- 其中优化回答和劣选回答的权重也不一定一直为1/0，可以根据分数赋予权重

### GRPO (Group Relative Policy Optimization，广义相对策略优化或群组相对策略优化)
- 前面这三种RL微调方法的本质理论差别其实在于“损失函数”，在于整体数学理论的引导
- GRPO 完全移除了 Critic 网络。它不是通过 Critic 来估计优势函数，而是采用群组相对优势估计
- 具体步骤
1. 对于同一个 Prompt $\mathbf{x}$，模型 $\pi_{\text{old}}$ 采样生成一个包含 $G$ 个候选回答的群组 $\{\mathbf{y}_1, \mathbf{y}_2, \ldots, \mathbf{y}_G\}$。
2. 使用奖励模型 (RM) 对这 $G$ 个回答分别进行评分，得到奖励 $\{R_1, R_2, \ldots, R_G\}$。
3. 计算这个群组的平均奖励 $\text{Avg}(R_{\text{group}})$ 作为基准线 (Baseline)。
4. 计算每个回答的优势函数 $\hat{A}_i$：$$\hat{A}_i = R_i - \text{Avg}(R_{\text{group}})$$
-  优势函数 $\hat{A}_i$ 不再是基于模型预测的绝对值，而是相对于同一 Prompt 下其他回答的相对好坏。如果回答 $\mathbf{y}_i$ 比同一批次的平均水平好，则 $\hat{A}_i > 0$；反之则 $\hat{A}_i < 0$。
- 关键在于每一轮Group之间的比较

### GRPO (Generalized Relative Policy Optimization，广义相对策略优化)
- DPO/GRPO（作为无 RM 偏好优化时）： 可以直接从偏好数据 $(\mathbf{y}_w, \mathbf{y}_l)$ 中优化策略，避免了奖励模型的误差。在这种情况下，GRPO 可以被视为 DPO 的一个更广义或替代性的损失函数，它仍然基于最大化优选回答的对数概率与劣选回答的对数概率之差。
- KL 散度作为惩罚项或约束项被加入到优化目标中，确保模型在最大化奖励的同时，不能离原始策略太远，避免策略崩塌

### Group Sequence Policy Optimization (GSPO)
- 将策略优化从代币级别（Token-level）提升到序列级别（Sequence-level）
- GSPO 定义了序列级别的重要性比率，直接基于整个序列的似然比 $\frac{\pi_{\theta}(\mathbf{y}|\mathbf{x})}{\pi_{\text{old}}(\mathbf{y}|\mathbf{x})}$（通常会进行长度归一化以减少方差）。这种方法将优化目标与奖励信号（奖励通常是针对整个序列的）更紧密地对齐，且提供了更稳定的离策略（Off-Policy）更新信号
- GSPO 的序列级方法，只依赖于整个序列的似然，这对于代币级别的专家激活变化具有更高的容忍度和鲁棒性，从而固有地解决了 MoE 模型 RL 训练的稳定性问题

### DAPO (Decoupled Clip and Dynamic sAmpling Policy Optimization，解耦裁剪与动态采样策略优化)
- 传统的 PPO 和 GRPO 使用对称裁剪范围 $[\mathbf{1}-\epsilon, \mathbf{1}+\epsilon]$ (例如 $\epsilon=0.2$) 来限制新旧策略的比率。研究发现，这种对称限制容易导致策略熵坍塌（Entropy Collapse），即模型过于确信某些答案而缺乏探索性
- DAPO 将裁剪范围解耦，使用非对称的裁剪界限 $[\mathbf{1}-\epsilon_{\text{low}}, \mathbf{1}+\epsilon_{\text{high}}]$。它保持较低的下限 $\epsilon_{\text{low}}$ (如 0.2)，以防止模型将低概率动作的概率降至 0。它提高上限 $\epsilon_{\text{high}}$ (如 0.28，即 Clip-Higher)，允许模型在奖励为正的方向上进行更大胆的策略更新，从而鼓励探索和多样性，有效防止熵坍塌
- 动态采样：在生成（采样）阶段，如果一个 Prompt 的所有生成结果的奖励（或准确率）都相同（例如都为 1 或都为 0），则将该 Prompt 过滤掉

### On-Policy RL Meets Off-Policy Experts: Harmonizing SFT and RL via Dynamic Weighting (CHORD)
- 先进行 SFT 再进行 RL (SFT-then-RL)是常用混合后训练方法，虽然直观，但在实践中常常表现不佳，甚至不如单纯的 RL。其根本原因在于，来自外部专家的“离策略(off-policy)”数据可能会严重干扰模型在 SFT 阶段已经建立的内部模式，导致模型性能下降，并可能在后续的 RL 阶段陷入过拟合
- CHORD 的核心思想是在强化学习过程中，动态融合专家数据（SFT），通过 全局权重 μ + token 级别权重 φ 的双重控制机制，在模仿与探索之间实现平衡
- CHORD 算法通过在 GRPO loss 中引入 SFT loss，实现动态混合训练
- 通过在训练过程中逐步 衰减 μ，实现从模仿专家到自主探索的过渡
- CHORD-ϕ 通过 token-wise 权重函数 φ 动态控制每个专家 token 的梯度贡献
- https://swift.readthedocs.io/zh-cn/latest/Instruction/GRPO/AdvancedResearch/CHORD.html